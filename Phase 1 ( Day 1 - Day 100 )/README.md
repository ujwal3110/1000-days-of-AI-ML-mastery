# ğŸš€ PHASE 1 
## ğŸ“˜ Day 01 â€” Vectors & Dot Product Fundamentals
Building AI From First Principles â€” One Day at a Time

Welcome to Day 1 of my 1000 Days of AI/ML Mastery.
Phase 1 is all about foundations, but not by reading theory â€”
ğŸ‘‰ I will build the foundations myself from scratch.

Today marks the beginning of implementing a mini-NumPy, a math engine that will later power neural networks, optimizers, attention layers, and full deep learning frameworks.

---

### ğŸ”¥ What I Learned Today
- What a vector really represents (geometry, magnitude, direction)
- Why the dot product measures alignment between vectors
(positive = same direction, negative = opposite, 0 = orthogonal)
- Scalar vs vector intuition
- Importance of vector operations in machine learning
(embeddings, gradients, projection, similarity, etc.) 

### ğŸ” Deeper Understanding
- Vectors are not just lists of numbers â€” they are arrows that capture both magnitude and direction, which makes them perfect for representing anything in machine learning that has structure or meaning.
- The dot product becomes powerful because it tells you how much one vector â€œpoints in the directionâ€ of another, which is exactly why it can measure similarity, alignment, or influence. When the dot product is positive, the vectors reinforce each other; when negative, they oppose each other; and when zero, they share nothing in common â€” this is the geometric basis of orthogonality.
- In machine learning, this concept shows up everywhere: embeddings use dot products to measure similarity, gradients use them to update weights, and attention mechanisms in Transformers rely on them to decide which tokens matter to each other. 
- Thinking in vectors helps you visualize models as systems moving through high-dimensional space, not just crunching numbers. Once this intuition becomes second nature, understanding deeper concepts like projections, cosine similarity, and attention becomes effortless

---
### ğŸ› ï¸ What I Built Today

#### ğŸ“ File
**Directory:** `foundations/math/`  
**File:** `vector.py` (example)

#### ğŸ”§ Implemented Functions

- `dot(a, b)`
- `add(a, b)`
- `sub(a, b)`
- `scale(a, Î±)`


###### â¡ï¸ This is the first building block for matrix multiplication, neural networks, and gradients.

#### ğŸ§ª Experiments

- Tested dot product with different vector orientations
- Verified geometric interpretation using angle formula
- Created a small notebook to plot 2D vectors and visualize dot product sign

Screenshot of notebook added to:

experiments/day1_vector_experiments.ipynb

#### ğŸ¯ Plan for Tomorrow

Implement vector operations:

- scalar multiply
- vector subtraction
- magnitude + normalization
- Finish complete Vector API

Start implementing Matrix class on Day 4

#### ğŸŒ… Daily Reflection

Today sets the foundation for the next 1000 days.
A simple vector is the seed from which LLMs, Transformers, and GPUs grow.
Building from first principles is the only way to become a true AI engineer.


## ğŸ“˜ Day 02 â€” Vector Magnitude Unit vectors & Normalization


### ğŸ”¥ What I Learned Today
âœ… 1. Vector Magnitude = The â€œlengthâ€ of information
The magnitude tells how â€œlargeâ€ a vector is â€” not just in geometry, but in meaning.
For embeddings, it relates to how strong or expressive a feature is.

âœ… 2. Normalization = Keeping vectors comparable
Turning a vector into a unit vector keeps direction but removes scale.
This is essential in ML because many algorithms depend only on direction, not raw values.

âœ… 3. Why unit vectors dominate ML
Unit vectors make:
- cosine similarity meaningful
- training more stable
- embeddings comparable across datasets
- attention scores easier to interpret

âœ… 4. Clipping, scaling & numerical stability

I learned that normalization must avoid division by zero â†’ an important detail for stable deep learning systems (e.g., layer normalization, RMSNorm).

### ğŸ” Deeper Understanding
ğŸ§­ Magnitude as energy
I started visualizing magnitude not just as length but as energy contained in a vector â€” useful for thinking about gradients.
A large gradient vector means â€œhigh corrective energy,â€ while a normalized gradient means â€œcontrolled, stable learning.â€

ğŸ”¦ Why normalization matters for embeddings
If two embedding vectors have different magnitudes, the dot product mixes up similarity with raw strength.
But after normalization:
similarity = pure direction comparison

This geometric purity is what powers cosine similarity, semantic search, LLM attention, and contrastive learning (CLIP, SimCLR, DINO, etc.).

ğŸ¯ Direction > magnitude in high-dimensional ML
Neural networks often care more about the orientation of a vector than its size.
This intuition is key to understanding:

- how Transformer attention picks important tokens
- how word embeddings capture meaning
- why norm-based regularization stabilizes training
When I saw this geometrically, everything felt much clearer.

---
### ğŸ› ï¸ What I Built Today
#### ğŸ“ File
**Directory:** `foundation/math/vector.py`- extended with full vector operations.
**File:** `vector.py` 

#### ğŸ”§ Implemented Functions

- `magnitude(a)`
- `normalize(a)`
- `distance(a, b)`
- `hadamard(a, b)`  # elementwise multiply
- `scale(a, Î±)`     # refined version with type checks



       
          

ğŸ’¡ Important Notes in Implementation

- normalization handles near-zero vectors safely
- magnitude uses stable âˆš(Î£ xÂ²) computation
- all operations enforce same-length inputs
- clean error messages â†’ useful for debugging ML code later


#### ğŸ§ª Experiments

ğŸ§¹ 1. Normalization tests
Verified that every normalized vector has magnitude â‰ˆ 1.0
Checked edge cases:
- zero vector
- extremely small values
- extremely large values

ğŸ“ 2. Distance vs dot product
Plotted pairs of vectors on a 2D grid:
- small angles â†’ small distance
- orthogonal vectors â†’ distance larger but dot product = 0
Great intuition for similarity metrics.

ğŸ¨ 3. Visual exploration notebook

It includes:
- rendering random vectors
- showing magnitude
- overlaying unit vectors
- coloring vectors based on dot product sign

This visual grounding is helping me feel vector math instead of just knowing formulas.

#### ğŸ¯ Plan for Tomorrow
Tomorrow will complete the Vector module and connect it to early neural-network mechanics.

Goals for Day 3
- implement projection of one vector onto another
- cosine similarity from scratch
- angle between vectors
- deeper geometric experiments
- start preparing the API surface for the upcoming Matrix class

This will complete the full Vector API, making it ready for:
matrix multiplication, forward passes, backprop, optimizers, embeddings, and attention scoring.

#### ğŸŒ… Daily Reflection

Today felt like unlocking a piece of the geometric engine inside every neural network.
Vectors arenâ€™t just arrays â€” theyâ€™re geometry encoded as numbers.
Magnitude gives them weight; normalization gives them identity.
Understanding this at a deep level means the math behind neural networks stops feeling abstract and starts feeling alive.

These fundamentals arenâ€™t glamorous, but they are the foundation on which entire AI systems stand.
Day 2 is done â€” and the base of my mini-NumPy engine is starting to take shape.







