# ğŸš€ PHASE 1 
## ğŸ“˜ Day 01 â€” Vectors & Dot Product Fundamentals
Building AI From First Principles â€” One Day at a Time

Welcome to Day 1 of my 1000 Days of AI/ML Mastery.
Phase 1 is all about foundations, but not by reading theory â€”
ğŸ‘‰ I will build the foundations myself from scratch.

Today marks the beginning of implementing a mini-NumPy, a math engine that will later power neural networks, optimizers, attention layers, and full deep learning frameworks.

---

### ğŸ”¥ What I Learned Today
- What a vector really represents (geometry, magnitude, direction)
- Why the dot product measures alignment between vectors
(positive = same direction, negative = opposite, 0 = orthogonal)
- Scalar vs vector intuition
- Importance of vector operations in machine learning
(embeddings, gradients, projection, similarity, etc.) 

### ğŸ” Deeper Understanding
- Vectors are not just lists of numbers â€” they are arrows that capture both magnitude and direction, which makes them perfect for representing anything in machine learning that has structure or meaning.
- The dot product becomes powerful because it tells you how much one vector â€œpoints in the directionâ€ of another, which is exactly why it can measure similarity, alignment, or influence. When the dot product is positive, the vectors reinforce each other; when negative, they oppose each other; and when zero, they share nothing in common â€” this is the geometric basis of orthogonality.
- In machine learning, this concept shows up everywhere: embeddings use dot products to measure similarity, gradients use them to update weights, and attention mechanisms in Transformers rely on them to decide which tokens matter to each other. 
- Thinking in vectors helps you visualize models as systems moving through high-dimensional space, not just crunching numbers. Once this intuition becomes second nature, understanding deeper concepts like projections, cosine similarity, and attention becomes effortless

---
### ğŸ› ï¸ What I Built Today

#### ğŸ“ File
**Directory:** `foundations/math/`  
**File:** `vector.py` (example)

#### ğŸ”§ Implemented Functions

- `dot(a, b)`
- `add(a, b)`
- `sub(a, b)`
- `scale(a, Î±)`


###### â¡ï¸ This is the first building block for matrix multiplication, neural networks, and gradients.

#### ğŸ§ª Experiments

- Tested dot product with different vector orientations
- Verified geometric interpretation using angle formula
- Created a small notebook to plot 2D vectors and visualize dot product sign

Screenshot of notebook added to:

experiments/day1_vector_experiments.ipynb

#### ğŸ¯ Plan for Tomorrow

Implement vector operations:

- scalar multiply
- vector subtraction
- magnitude + normalization
- Finish complete Vector API

Start implementing Matrix class on Day 4

#### ğŸŒ… Daily Reflection

Today sets the foundation for the next 1000 days.
A simple vector is the seed from which LLMs, Transformers, and GPUs grow.
Building from first principles is the only way to become a true AI engineer.


## ğŸ“˜ Day 02 â€” Vector Magnitude Unit vectors & Normalization


### ğŸ”¥ What I Learned Today
âœ… 1. Vector Magnitude = The â€œlengthâ€ of information
The magnitude tells how â€œlargeâ€ a vector is â€” not just in geometry, but in meaning.
For embeddings, it relates to how strong or expressive a feature is.

âœ… 2. Normalization = Keeping vectors comparable
Turning a vector into a unit vector keeps direction but removes scale.
This is essential in ML because many algorithms depend only on direction, not raw values.

âœ… 3. Why unit vectors dominate ML
Unit vectors make:
- cosine similarity meaningful
- training more stable
- embeddings comparable across datasets
- attention scores easier to interpret

âœ… 4. Clipping, scaling & numerical stability

I learned that normalization must avoid division by zero â†’ an important detail for stable deep learning systems (e.g., layer normalization, RMSNorm).

### ğŸ” Deeper Understanding
ğŸ§­ Magnitude as energy
I started visualizing magnitude not just as length but as energy contained in a vector â€” useful for thinking about gradients.
A large gradient vector means â€œhigh corrective energy,â€ while a normalized gradient means â€œcontrolled, stable learning.â€

ğŸ”¦ Why normalization matters for embeddings
If two embedding vectors have different magnitudes, the dot product mixes up similarity with raw strength.
But after normalization:
similarity = pure direction comparison

This geometric purity is what powers cosine similarity, semantic search, LLM attention, and contrastive learning (CLIP, SimCLR, DINO, etc.).

ğŸ¯ Direction > magnitude in high-dimensional ML
Neural networks often care more about the orientation of a vector than its size.
This intuition is key to understanding:

- how Transformer attention picks important tokens
- how word embeddings capture meaning
- why norm-based regularization stabilizes training
When I saw this geometrically, everything felt much clearer.

---
### ğŸ› ï¸ What I Built Today
#### ğŸ“ File
**Directory:** `foundation/math/vector.py`- extended with full vector operations.
**File:** `vector.py` 

#### ğŸ”§ Implemented Functions

- `magnitude(a)`
- `normalize(a)`
- `distance(a, b)`
- `hadamard(a, b)`  # elementwise multiply
- `scale(a, Î±)`     # refined version with type checks



       
          

ğŸ’¡ Important Notes in Implementation

- normalization handles near-zero vectors safely
- magnitude uses stable âˆš(Î£ xÂ²) computation
- all operations enforce same-length inputs
- clean error messages â†’ useful for debugging ML code later


#### ğŸ§ª Experiments

ğŸ§¹ 1. Normalization tests
Verified that every normalized vector has magnitude â‰ˆ 1.0
Checked edge cases:
- zero vector
- extremely small values
- extremely large values

ğŸ“ 2. Distance vs dot product
Plotted pairs of vectors on a 2D grid:
- small angles â†’ small distance
- orthogonal vectors â†’ distance larger but dot product = 0
Great intuition for similarity metrics.

ğŸ¨ 3. Visual exploration notebook

It includes:
- rendering random vectors
- showing magnitude
- overlaying unit vectors
- coloring vectors based on dot product sign

This visual grounding is helping me feel vector math instead of just knowing formulas.

#### ğŸ¯ Plan for Tomorrow
Tomorrow will complete the Vector module and connect it to early neural-network mechanics.

Goals for Day 3
- implement projection of one vector onto another
- cosine similarity from scratch
- angle between vectors
- deeper geometric experiments
- start preparing the API surface for the upcoming Matrix class

This will complete the full Vector API, making it ready for:
matrix multiplication, forward passes, backprop, optimizers, embeddings, and attention scoring.

#### ğŸŒ… Daily Reflection

Today felt like unlocking a piece of the geometric engine inside every neural network.
Vectors arenâ€™t just arrays â€” theyâ€™re geometry encoded as numbers.
Magnitude gives them weight; normalization gives them identity.
Understanding this at a deep level means the math behind neural networks stops feeling abstract and starts feeling alive.

These fundamentals arenâ€™t glamorous, but they are the foundation on which entire AI systems stand.
Day 2 is done â€” and the base of my mini-NumPy engine is starting to take shape.


## ğŸ“˜ Day 03 â€” Matrix Foundations, Shapes & Linear Algebra Core


### ğŸ”¥ What I Learned Today
âœ… 1. A Matrix is just a collection of aligned vectors
Rows must have equal length â€” this enforcement is the bedrock of linear algebra.
I finally understood that shape is the first contract that makes every ML operation possible.

âœ… 2. Why matrices matter in machine learning
Matrices represent:
- weights in neural networks
- batches of embeddings
- transformation operators
- attention score maps
- convolution lowering (im2col)
Everything deep learning does is basically matrix operations at scale.

âœ… 3. Matrix multiplication = combining directions
- A * B is not just loops â€” it's:
- projecting rows of A
- onto columns of B
- and building new geometry
This geometric interpretation reveals why linear layers, transformers, and even CNNs rely on the same primitive.

âœ… 4. Transpose is more important than I thought
Transposing flips:
- row vectors â†’ column vectors
- column vectors â†’ row vectors
- Attention, backprop, similarity searchâ€¦ all rely heavily on fast, clean transposes.

### ğŸ” Deeper Understanding
ğŸ§­ Matrix as a transformation machine
A matrix turns an input vector into a transformed vector:
scale â†’ rotate â†’ shift â†’ distort â†’ embed â†’ classify.
Seeing matrices as operators instead of just nested arrays changed how I think about forward passes.

ğŸ”¦ Why shape correctness is critical
Every ML failure â€” exploding gradients, invalid losses, NaN propagation â€” often starts with shape mismatch.
Now I enforce:
- consistent row sizes
- compatible dimensions for multiplication
- clear, helpful error messages
This is identical to how PyTorch/NumPy do internal validation.

ğŸ¯ Matrix multiplication â‰  elementwise multiply
Elementwise = Hadamard
Matrix multiply = Linear transformation

This distinction becomes crucial when:
- building embeddings
- computing attention
- implementing backprop

Understanding both at a geometric level gave me clarity.
---
### ğŸ› ï¸ What I Built Today
#### ğŸ“ File
**Directory:** `foundation/math/matrix.py`
**File:** `matrix.py` 

#### ğŸ”§ Implemented Functions

- `magnitude(a)`
- `shape()`
- `add(A, B)`
- `sub(A, B)`
- `scale(A, Î±)`
- `hadamard(A, B)`
- `transpose(A)`
- `matvec(A, v)` â€” matrix Ã— vector
- `matmul(A, B)` â€” matrix Ã— matrix


ğŸ’¡ Important Notes in Implementation

- strict 2D shape checks for safety
- type coercion to float for numerical consistency
- matrix-vector multiply uses dot products row-wise
- matrix-matrix multiply uses transpose trick for cleaner implementation
- clean, NumPy-like error messages for debugging
- zero dependency, pure Python implementation â€” educational & transparent


#### ğŸ§ª Experiments


ğŸ§© 1. Verified shapes across all operations
Tested:
- correct alignment
- mismatched sizes
- rectangular matrices
- multiplication shape rules

This helped solidify the mental model for (mÃ—n) Â· (nÃ—p) â†’ (mÃ—p).

ğŸ“ 2. Visualizing matrix â†’ vector transformations
Plotted simple 2D vectors under various matrices:
- scaling
- rotation
- shear

This made matrix multiplication feel intuitive rather than symbolic.

ğŸ”— 3. Compared matrix multiplication vs Hadamard
Saw clearly how:
- Hadamard = filters features
- Matmul = remaps features
A crucial insight for understanding deep learning architecture internals.

ğŸ“Š 4. Benchmarked naive matmul
Measured time complexity and confirmed the O(nÂ³) cost of the pure Python version â€” great context for why BLAS, CUDA, and Tensor Cores matter.

ğŸ¯ Plan for Tomorrow
Day 4 Goals
- implement matrix norms
- row/column slicing
- outer product
- broadcast engine (mini-NumPy style)
- prepare API for backprop & neural network layers
This will push the project closer to a fully working â€œmicro-NumPyâ€ core with clean vector + matrix interop.

ğŸŒ… Daily Reflection
Today felt like I unlocked the transformation engine behind neural networks.
Matrices aren't storage â€” they are machines that transform geometry.
Understanding shapes, transposes, and multiplication gave me the mental model to build linear layers, attention, gradients, and optimization from scratch.
Slow steps, but foundational steps.

Day 3 complete â€” and the heart of my mini-NumPy engine is now beating.


## ğŸ“˜ Day 04 â€” Matrix Norms, Slicing, Outer Products & Broadcasting Engine


### ğŸ”¥ What I Learned Today
ğŸ”¥ What I Learned Today

âœ… 1. Matrix norms are the â€œsizeâ€ of transformations
Just like vector magnitude measures energy, matrix norms measure:
- total transformation strength
- stability of neural networks
- gradient explosion risk
I implemented Frobenius Norm, the matrix equivalent of vector magnitude.

âœ… 2. Row/column slicing = reading structure
Neural networks access:
- each row as a neuron
- each column as feature dimension
Understanding clean slicing makes linear algebra intuitive and prepares the architecture for batched operations.

âœ… 3. Outer product = building matrices from vectors
Outer product forms the basis of:
- attention score matrices
- covariance matrices
- weight updates in gradient descent
- rank-1 approximations

It is literally:
geometry Ã— geometry = transformation

âœ… 4. Broadcasting = the secret sauce of NumPy
Broadcasting rules:
- align shapes from the right
- expand dimensions of size 1
- operations apply across expanded dimensions
  
I built a simplified broadcasting engine that supports:
- matrix + scalar
- matrix + row vector
- matrix + column vector
- matrix + matrix (same or broadcastable shapes)

This small feature unlocks 70% of NumPyâ€™s magic.

### ğŸ” Deeper Understanding

ğŸ§­ Frobenius norm as transformation power
I learned that Frobenius norm approximates how much a matrix can stretch space.
- High norm â†’ unstable gradients
- Low norm â†’ smoother training

ğŸ”¦ Why slicing matters
I now think of matrices as:
- rows = samples
- columns = features
This intuition becomes crucial when building:
- dense layers
- embeddings
- loss functions
- statistical preprocessing

ğŸ¯ Broadcasting = fewer loops, more math
Transformers rely heavily on broadcasting for:
- positional encodings
- attention score scaling
- mask expansion
- batch operations

My tiny broadcasting system is now good enough to support future neural-network layers.
---

### ğŸ› ï¸ What I Built Today
#### ğŸ“ File
**Directory:** `foundation/math/matrix.py`
**File:** `vector.py` (extended)
**File:** `matrix.py` (added features)


#### ğŸ§ª Experiments

1. Norm stability tests
Verified numeric stability using:
- tiny values
- huge values
- random matrices

2. Slicing visualizations

- Plotted each row as a separate vector.
- Saw how features separate by columns.
- Very helpful for visualizing embeddings.

3. Outer product intuition tests

Generated:
- key Ã— query
- noise Ã— noise
- feature Ã— weight
Saw how outer products form structured rank-1 matrices.

4. Broadcasting correctness
Tested:
- scalar + matrix
- matrix + row-vector
- matrix + column-vector
- incompatible shapes (error raised correctly)

ğŸ¯ Plan for Tomorrow
- build a full Dense Layer (weights, bias, forward)
- implement activation functions (ReLU, sigmoid, tanh)
- design a tiny autograd engine
- set up a mini backprop pipeline
- start preparing for optimizers (SGD)
This will mark the transition from pure math â†’ real neural-network components.

ğŸŒ… Daily Reflection

Today I built the mathematical machinery behind real deep-learning pipelines.
Matrix norms taught me how networks stay stable, slicing revealed structure within data, outer products showed how attention scores are formed, and broadcasting unlocked vectorized computation.

Each feature felt like adding a new muscle to my mini-NumPy engine â€” stronger, more flexible, and ready for neural network layers.

Day 4 done â€” and the micro-NumPy engine is starting to feel alive.

